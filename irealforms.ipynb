{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, unquote\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_valid_url(url, base_url):\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.scheme) and bool(parsed.netloc) and parsed.netloc == urlparse(base_url).netloc\n",
    "\n",
    "def is_not_php(url):\n",
    "    return '.php' not in url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_valid_urls_from_website(website_url, visited=None, max_requests=10):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    if website_url in visited or len(visited) >= max_requests:\n",
    "        return []\n",
    "\n",
    "    print(f\"Visiting: {website_url}\")\n",
    "\n",
    "    try:\n",
    "        # Send a request to the website\n",
    "        response = requests.get(website_url)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "\n",
    "        # Add the URL to the set of visited URLs\n",
    "        visited.add(website_url)\n",
    "\n",
    "        # Parse the content of the website\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract all anchor tags with href attributes\n",
    "        anchors = soup.find_all('a', href=True)\n",
    "\n",
    "        # Generate a list of valid irealb:// URLs\n",
    "        valid_urls = []\n",
    "        for anchor in anchors:\n",
    "            href = anchor.get('href')\n",
    "            if href.startswith('irealb://') and is_not_php(href):\n",
    "                valid_urls.append({'url': href, 'source_page': website_url})\n",
    "\n",
    "        # Traverse through other links on the page recursively\n",
    "        for anchor in anchors:\n",
    "            href = anchor.get('href')\n",
    "            full_url = urljoin(website_url, href)\n",
    "            if is_valid_url(full_url, website_url) and full_url not in visited:\n",
    "                valid_urls.extend(get_valid_urls_from_website(full_url, visited, max_requests))\n",
    "                if len(visited) >= max_requests:\n",
    "                    break\n",
    "                time.sleep(1)  # To avoid overwhelming the server\n",
    "\n",
    "        return valid_urls\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_to_dataframe(urls, file_name):\n",
    "    # Load existing data if file exists\n",
    "    if os.path.exists(file_name):\n",
    "        df_existing = pd.read_csv(file_name)\n",
    "    else:\n",
    "        df_existing = pd.DataFrame(columns=['url', 'source_page'])\n",
    "\n",
    "    # Convert the list of URLs to a DataFrame\n",
    "    df_new = pd.DataFrame(urls)\n",
    "\n",
    "    # Concatenate with existing data and remove duplicates\n",
    "    df_combined = pd.concat([df_existing, df_new]).drop_duplicates()\n",
    "\n",
    "    # Save the combined DataFrame back to the CSV file\n",
    "    df_combined.to_csv(file_name, index=False)\n",
    "    return df_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "website_url = 'https://www.irealb.com/forums/showthread.php?8483-Pop-400'\n",
    "max_requests = 250  # Limit the total number of requests\n",
    "output_file = 'irealb_urls.csv'  # Output file to save the results\n",
    "\n",
    "\n",
    "def seed_urls():\n",
    "    if os.path.exists(output_file):\n",
    "        df_final = pd.read_csv(output_file)\n",
    "        if os.path.getsize(output_file) > 0:\n",
    "            df_final = pd.read_csv(output_file)\n",
    "    else:\n",
    "        # Get valid URLs from the website\n",
    "        valid_urls = get_valid_urls_from_website(website_url, max_requests=max_requests)\n",
    "\n",
    "        # # Save the URLs to a CSV file and update it\n",
    "        df_final = save_to_dataframe(valid_urls, output_file)\n",
    "\n",
    "    # Load the DataFrame from the CSV file\n",
    "    df_final = pd.read_csv(output_file)\n",
    "    return df_final\n",
    "\n",
    "df_final = seed_urls()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_metadata(url: str) -> pd.DataFrame:\n",
    "    # Remove the scheme part (irealb://) from the URL\n",
    "    url = url.split(\"irealb://\")[-1]\n",
    "    \n",
    "\n",
    "    # Split the fields by \"==\" (for separating the main parts)\n",
    "    fields = url.split(\"==\")\n",
    "\n",
    "    # Decode any URL encoded characters\n",
    "    fields = [unquote(field) for field in fields]\n",
    "\n",
    "    # Assuming the fields correspond to specific metadata, we'll label them\n",
    "    data = {\n",
    "        \"Song Title=Artist\": fields[0] if len(fields) > 0 else None,\n",
    "        \"Style=Key\": fields[1] if len(fields) > 1 else None,\n",
    "        \"Unknown1\": fields[2] if len(fields) > 2 else None,\n",
    "        \"Unknown2\": fields[3] if len(fields) > 3 else None,\n",
    "        \"Unknown3\": fields[4] if len(fields) > 4 else None,\n",
    "        \"Unknown4\": fields[5] if len(fields) > 5 else None,\n",
    "        \"Unknown5\": fields[6] if len(fields) > 6 else None,\n",
    "        \"Unknown6\": fields[7] if len(fields) > 7 else None\n",
    "    }\n",
    "    try:\n",
    "        data[\"Song Title\"], data[\"Artist\"] = data[\"Song Title=Artist\"].split(\"=\")\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        data[\"Style\"], data[\"Key\"] = data[\"Style=Key\"].split(\"=\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame([data])\n",
    "\n",
    "    # order the columns\n",
    "    df = df[[\"Song Title=Artist\", \"Style=Key\", \"Unknown1\", \"Unknown2\", \"Unknown3\", \"Unknown4\", \"Unknown5\", \"Unknown6\"]]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove row from df if url starts with irealb://%\n",
    "df_final = df_final[~df_final['url'].str.startswith('irealb://%')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.concat([parse_metadata(url) for url in df_final['url']], ignore_index=True)\n",
    "\n",
    "df_combined = pd.concat([df_final, df_meta], axis=1)\n",
    "\n",
    "df_combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.drop_duplicates(subset=['url'], inplace=True)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_partial = df_final.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write each entry of data frame column to a file with .ireal extension if the song isn't in the file\n",
    "with open(\"src/static/gathered_songs.ireal\", 'r') as f:\n",
    "    gathered_songs = f.read().splitlines()\n",
    "\n",
    "for i, url in enumerate(df_partial['url']):\n",
    "    if url not in gathered_songs:\n",
    "        with open(\"src/static/gathered_songs.ireal\", 'a') as f:\n",
    "            f.write(str(url) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
